version: '3'

services:
  stablediff:
    build:
      context: .
      dockerfile: Dockerfile.stablediffusion
    image: stablediff:local # local tag
    container_name: stablediff
    # NOTE: this is a different way to handle gpus
    # than the LLM stuff... maybe a better idea to do things
    # this way but I'll cargo cult both for now
    # {% if 'ai-gpu' in salt['grains.get']('roles') %}
    runtime: nvidia
    # {% endif %}
    environment:
      SALTPADDING: "throwaway"
      # {% if 'ai-gpu' in salt['grains.get']('roles') %}
      NVIDIA_VISIBLE_DEVICES: all
      # {% endif %}
    entrypoint:
      [
        "python",
        "launch.py",
        "--listen",
        "--no-download-sd-model",
        # {% if 'ai-gpu' not in salt['grains.get']('roles') %}
        "--xformers",
        "--use-cpu",
        "all",
        "--precision",
        "full",
        "--no-half",
        "--skip-torch-cuda-test" # {% endif %}
      ]
    # command: >
    #   ". /stablediff.env; echo launch.py $$COMMANDLINE_ARGS; if [ ! -d /stablediff-web/.git ]; then
    #     cp -a /sdtemp/. /stablediff-web/
    #   fi; #if [ ! -f /stablediff-web/models/Stable-diffusion/*.ckpt ]; then #  echo 'Please copy stable diffusion model to stablediff-models directory' #  echo 'You may need sudo to perform this action' #  exit 1 #fi; python launch.py"
    ports:
      - "7860:7860"
    volumes:
      - ./stablediff-web:/stablediff-web
      - ./models:/stablediff-web/models/Stable-diffusion
